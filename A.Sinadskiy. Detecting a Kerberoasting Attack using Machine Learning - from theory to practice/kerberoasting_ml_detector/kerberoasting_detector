#!/usr/bin/env python3
from argparse import ArgumentParser
from collections import OrderedDict
from csv import DictReader, writer
from dataclasses import asdict, dataclass, fields
from datetime import datetime, timedelta
from numpy import array
from os.path import join
from scapy.layers.inet import IP
from scapy.layers.kerberos import KRB_TGS_REQ, EncryptedData, KRB_Ticket, KRB_AP_REQ, PADATA, KRB_KDC_REQ_BODY, \
    KRB_AS_REQ
from scapy.packet import Packet
from scapy.sendrecv import sniff
from schedule import every, run_pending
from scipy.stats import kstest
from signal import signal, SIGTERM
from sklearn.metrics import *
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from syslog import LOG_WARNING, syslog, LOG_INFO
from threading import Thread
from time import perf_counter, sleep


@dataclass
class Record:
    time: datetime
    type: str
    source: str
    cname: str | None
    sname: str
    realm: str
    flood_mark: int | None
    fixed_time_mark: int | None

@dataclass
class FrequenciesItem:
    number: int
    source_sname_pairs: set[tuple[str, str]]
    flood_marks: int


rc4_hmac_signature = 23
lof_min = -1.1
records: list[Record]
frequencies: OrderedDict[datetime, FrequenciesItem]
user_timestamps: dict[str, dict[datetime, int]]
running = True
svm: OneClassSVM
start_time = datetime.now().timestamp()


def handle_sigterm(signum, frame):
    dump_dataset()
    global running
    running = False
    exit(0)


def run_schedule():
    while running:
        run_pending()
        sleep(1)


def dump_dataset():
    timestamp = int(datetime.now().timestamp())  # количество секунд с начала эпохи (01.01.1970)
    global records
    if len(records) > 0:
        with open(join('datasets', f'{timestamp}dataset.csv'), 'w') as dataset_file:
            csv_writer = writer(dataset_file)
            csv_writer.writerow([f.name for f in fields(Record)])
            record_fields = [[value for name, value in asdict(record).items()] for record in records]
            csv_writer.writerows(record_fields)
        request_number = len(records)
        records = []
        for_removal = []
        for timestamp in frequencies:
            if timestamp.timestamp() > start_time:
                for_removal.append(timestamp)
        for item in for_removal:
            frequencies.pop(item)
        for user, timestamps_item in user_timestamps.items():
            for_removal = []
            for stamp in timestamps_item.keys():
                if stamp.timestamp() > start_time:
                    for_removal.append(stamp)
            for item in for_removal:
                timestamps_item.pop(item)
        syslog(LOG_INFO, f'Сделан дамп запросов в файл {timestamp}dataset.csv '
                         f'(количество запросов: {request_number})')
    else:
        syslog(LOG_INFO, f'Дамп запросов не сделан, т.к. не было запросов')


def load_dataset():
    loaded_records = []
    with open(join('datasets', 'sample.csv')) as dataset_file:
        csv_reader = DictReader(dataset_file)
        for row in csv_reader:
            record = Record(**row)
            # необходимо преобразовать в datetime, т.к. csv_reader прочитает datetime как строку
            record.time = datetime.strptime(record.time, '%Y-%m-%d %H:%M:%S.%f')
            record.flood_mark = 0 if record.flood_mark == '' else int(record.flood_mark)
            record.fixed_time_mark = 0 if record.fixed_time_mark == '' else int(record.fixed_time_mark)
            loaded_records.append(record)
    freqs = OrderedDict()
    timestamps = {}
    for record in loaded_records:
        time_without_seconds = record.time.replace(microsecond=0)
        if time_without_seconds in freqs.keys():
            freqs[time_without_seconds].number += 1
            freqs[time_without_seconds].source_sname_pairs.add((record.source, record.sname))
            freqs[time_without_seconds].flood_marks += record.flood_mark
        else:
            freqs[time_without_seconds] = FrequenciesItem(1, {(record.source, record.sname)}, record.flood_mark)
        if record.source in timestamps.keys():
            if time_without_seconds in timestamps[record.source].keys():
                timestamps[record.source][time_without_seconds] += record.fixed_time_mark
            else:
                timestamps[record.source][time_without_seconds] = record.fixed_time_mark
        else:
            timestamps[record.source] = {time_without_seconds: record.fixed_time_mark}
    syslog(LOG_INFO, f'Датасет загружен')
    global svm
    svm = OneClassSVM(nu=1e-5, gamma=0.1)
    frequencies_vectors = [(item.number,
                            len(item.source_sname_pairs) / len(set([pair[0] for pair in item.source_sname_pairs])))
                            for item in freqs.values()]
    time_start = perf_counter()
    flood_prediction = svm.fit_predict(frequencies_vectors)
    time_end = perf_counter()
    syslog(LOG_INFO, f'SVM обучена за {time_end - time_start} секунд')
    flood_marks = [1 if item.flood_marks == 0 else -1 for item in freqs.values()]
    write_learning_metrics(flood_marks, flood_prediction, 'flood_metrics.txt')
    lof = LocalOutlierFactor(n_neighbors=50)
    fixed_time_prediction = [get_fixed_time_marks(lof, list(timestamps[user].keys())) for user in timestamps]
    write_learning_metrics([1 if sum(timestamps[user].values()) == 0 else -1 for user in timestamps.keys()],
                           fixed_time_prediction,
                           'fixed_time_metrics.txt')
    return freqs, timestamps


def get_fixed_time_marks(lof: LocalOutlierFactor, date_times: list[datetime]):
    attack_marks = 0
    if len(date_times) >= 50: # если меньше 50, то LOF и тест Колмогорова-Смирнова плохо работают
        timestamps = array([date_time.timestamp() for date_time in date_times]).reshape(-1, 1)
        prediction = lof.fit_predict(timestamps)
        # если есть атака регулярными запросами, то сформируется ядро значений, для которых LOF вернет 1
        # обычные действия пользователя вернут -1
        # но нужно проверить, что на подвыборке тех значений, для которых LOF = 1, наблюдается равномерное распределение
        filter_prediction = prediction == 1
        suspected_attack_timestamps = timestamps[filter_prediction].reshape(1, -1)[0]
        max_timestamp = max(suspected_attack_timestamps)
        min_timestamp = min(suspected_attack_timestamps)
        kstest_result = kstest([timestamp - min_timestamp for timestamp in suspected_attack_timestamps],
                                'uniform',
                                args=(0, max_timestamp - min_timestamp))
        attack_marks = 0 if kstest_result.pvalue < 0.05 else len(suspected_attack_timestamps)
    return 1 if attack_marks == 0 else -1


def write_learning_metrics(attack_marks: list[int], prediction: list[int], file_name: str):
    with open(file_name, 'w') as metrics_file:
        metrics_file.write(f'Accuracy score: {accuracy_score(attack_marks, prediction)}\n')
        metrics_file.write(f'Classification report:\n{classification_report(attack_marks, prediction, digits=4, labels=[1, -1])}\n')
    syslog(LOG_INFO, f'Метрики обучения записаны в файл {file_name}')


def find_anomaly():
    # если в течение последней секунды не зафиксировано никакой активности, то и искать аномалии бессмысленно
    time = datetime.now()
    previous_second = time.replace(microsecond=0) - timedelta(seconds=1)
    if previous_second not in frequencies.keys():
        return
    frequencies_vectors = [(item.number, len(item.source_sname_pairs)) for item in frequencies.values()]
    last_measurement = svm.decision_function([frequencies_vectors[-1]])
    if last_measurement < 0:
        syslog(LOG_WARNING, f'Обнаружено аномально большое количество пакетов.')
    lof_for_timestamps = LocalOutlierFactor()
    for user in user_timestamps:
        date_times = user_timestamps[user].keys()
        if previous_second not in date_times or len(date_times) < 50: # если меньше 50, то LOF и тест Колмогорова-Смирнова плохо работают
            continue
        timestamps = array([date_time.timestamp() for date_time in date_times]).reshape(-1, 1)
        prediction = lof_for_timestamps.fit_predict(timestamps)
        # если есть атака регулярными запросами, то сформируется ядро значений, для которых LOF вернет 1
        # обычные действия пользователя вернут -1
        # но нужно проверить, что на подвыборке тех значений, для которых LOF = 1, наблюдается равномерное распределение
        filter_prediction = prediction == 1
        suspected_attack_timestamps = timestamps[filter_prediction].reshape(1, -1)[0]
        max_timestamp = max(suspected_attack_timestamps)
        min_timestamp = min(suspected_attack_timestamps)
        kstest_result = kstest([timestamp - min_timestamp for timestamp in suspected_attack_timestamps],
                               'uniform',
                               args=(0, max_timestamp-min_timestamp))
        if kstest_result.pvalue >= 0.05:
            syslog(LOG_WARNING, f'Обнаружена подозрительная серия регулярных запросов с хоста {user}')


def process_pack(pack: Packet):
    time = datetime.now()
    if pack.haslayer(KRB_TGS_REQ):
        req = pack[KRB_TGS_REQ]
    elif pack.haslayer(KRB_AS_REQ):
        req = pack[KRB_AS_REQ]
    else:
        syslog(LOG_INFO, 'Обнаружен неопознанный пакет, содержащий KRB-KDC-REQ-BODY. Пропускаем')
        return
    krb_kdc_req_body = pack[KRB_KDC_REQ_BODY]
    cname = krb_kdc_req_body.cname
    cname_str = None if cname is None else '/'.join(['' if part is None else part.val.decode() for part in cname.nameString])
    sname_str = '/'.join(['' if part is None else part.val.decode() for part in krb_kdc_req_body.sname.nameString])
    source = pack[IP].src
    realm = krb_kdc_req_body.realm.val.decode()
    syslog(LOG_INFO, f'Получен {req.summary()}. Отправитель: {source} '
                     f'(клиент {cname_str}). '
                     f'Запрашиваемый сервис: {sname_str}. '
                     f'Realm: {krb_kdc_req_body.realm.val.decode()}')
    records.append(Record(time, req.summary(), source, cname_str, sname_str, realm, None, None))
    time_without_seconds = time.replace(microsecond=0)
    if time_without_seconds in frequencies.keys():
        frequencies[time_without_seconds].number += 1
        frequencies[time_without_seconds].source_sname_pairs.add((source, sname_str))
    else:
        frequencies[time_without_seconds] = FrequenciesItem(1, {(source, sname_str)}, 0)
    if source not in user_timestamps.keys():
        user_timestamps[source] = {time_without_seconds: 0}
    elif time_without_seconds not in user_timestamps[source].keys():
        user_timestamps[source][time_without_seconds] = 0
    # обнаруживаем шифрование слабым алгоритмом RC4 (etype = 0x17 = 23)
    # находим алгоритм шифрования у TGT
    krb_ticket_etype = req[KRB_Ticket][EncryptedData].etype.val \
        if req.haslayer(KRB_Ticket) and req[KRB_Ticket].haslayer(EncryptedData) else None
    # находим алгоритм шифрования во вложенном AP_REQ
    krb_ap_req_etype = req[KRB_AP_REQ][EncryptedData].etype.val \
        if req.haslayer(KRB_AP_REQ) and req[KRB_AP_REQ].haslayer(EncryptedData) else None
    # находим алгоритм шифрования в PADATA
    padata_etype = req[PADATA][EncryptedData].etype.val \
        if req.haslayer(PADATA) and req[PADATA].haslayer(EncryptedData) else None
    if (krb_ticket_etype == rc4_hmac_signature or krb_ap_req_etype == rc4_hmac_signature or
            padata_etype == rc4_hmac_signature):
        syslog(LOG_WARNING, 'Обнаружен пакет, зашифрованный алгоритмом RC4.')


if __name__ == '__main__':
    signal(SIGTERM, handle_sigterm)
    parser = ArgumentParser(description='Детектор атаки Kerberoasting')
    parser.add_argument('--iface', required=False, help='Отслеживаемый интерфейс')
    args = parser.parse_args()
    records = []
    frequencies, user_timestamps = load_dataset()
    every().day.at('00:00').do(dump_dataset)
    every().second.do(find_anomaly)
    scheduler = Thread(target=run_schedule)
    scheduler.start()
    iface = args.iface
    syslog(LOG_INFO, f'Детектор атаки Kerberoasting запущен.')
    syslog(LOG_INFO, f'Запущено прослушивание интерфейса {iface}.')
    sniff(lfilter=lambda pack: pack.haslayer(KRB_KDC_REQ_BODY),
          prn=lambda pack: process_pack(pack),
          iface=iface)
